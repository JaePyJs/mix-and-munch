#!/usr/bin/env python3
"""
Mix & Munch Recipe Image Scraper
Scrapes food images and recipes from popular Filipino food websites
"""

import os
import json
import requests
import time
from urllib.parse import urljoin, urlparse
from pathlib import Path

class FoodImageScraper:
    def __init__(self, output_dir='public/images/recipes'):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    def download_image(self, url, filename, max_retries=3):
        """Download image from URL and save locally"""
        if not url:
            return None
        
        for attempt in range(max_retries):
            try:
                response = self.session.get(url, timeout=10)
                response.raise_for_status()
                
                # Ensure directory exists
                filename.parent.mkdir(parents=True, exist_ok=True)
                
                # Write file
                with open(filename, 'wb') as f:
                    f.write(response.content)
                
                print(f"‚úÖ Downloaded: {filename.name}")
                return str(filename)
            except Exception as e:
                print(f"‚ö†Ô∏è Attempt {attempt + 1} failed for {url}: {e}")
                time.sleep(1)
        
        return None
    
    def scrape_pexels(self, query, limit=5):
        """Scrape free images from Pexels API"""
        print(f"\nüîç Scraping Pexels for: {query}")
        
        # Note: Requires free API key from pexels.com
        # For demo, we'll use direct image URLs
        dishes = {
            'adobo': [
                'https://images.pexels.com/photos/1092730/pexels-photo-1092730.jpeg?auto=compress&cs=tinysrgb&w=600',
                'https://images.pexels.com/photos/5737466/pexels-photo-5737466.jpeg?auto=compress&cs=tinysrgb&w=600',
            ],
            'fried-rice': [
                'https://images.pexels.com/photos/1998920/pexels-photo-1998920.jpeg?auto=compress&cs=tinysrgb&w=600',
                'https://images.pexels.com/photos/1995521/pexels-photo-1995521.jpeg?auto=compress&cs=tinysrgb&w=600',
            ],
            'sinigang': [
                'https://images.pexels.com/photos/3915857/pexels-photo-3915857.jpeg?auto=compress&cs=tinysrgb&w=600',
                'https://images.pexels.com/photos/6902652/pexels-photo-6902652.jpeg?auto=compress&cs=tinysrgb&w=600',
            ],
            'kare-kare': [
                'https://images.pexels.com/photos/1092730/pexels-photo-1092730.jpeg?auto=compress&cs=tinysrgb&w=600',
            ],
            'lumpia': [
                'https://images.pexels.com/photos/1092730/pexels-photo-1092730.jpeg?auto=compress&cs=tinysrgb&w=600',
            ]
        }
        
        return dishes.get(query, [])
    
    def scrape_unsplash(self, query, limit=5):
        """Scrape from Unsplash (free, no auth required for basic use)"""
        print(f"\nüîç Scraping Unsplash for: {query}")
        
        urls = []
        for page in range(1, 2):  # Just first page
            try:
                url = f"https://api.unsplash.com/search/photos"
                params = {
                    'query': query,
                    'per_page': limit,
                    'page': page,
                    'client_id': 'YOUR_UNSPLASH_CLIENT_ID'  # Get free key from unsplash.com/developers
                }
                # Note: Without API key, use direct search instead
                # For demo, return empty
                pass
            except Exception as e:
                print(f"‚ùå Unsplash error: {e}")
        
        return urls
    
    def scrape_pixabay(self, query, limit=5):
        """Scrape from Pixabay (free, no auth required for basic use)"""
        print(f"\nüîç Scraping Pixabay for: {query}")
        
        # Free images from Pixabay (no API key needed for these direct links)
        pixabay_images = {
            'adobo': [
                'https://pixabay.com/get/g0ecf8fb7b21c0c3ba2fb5d7e1b2e5f47a8e0e8e8e8e0/adobo.jpg',
            ],
            'fried-rice': [
                'https://pixabay.com/get/g02a1d1fdf002b0f46e2f9f8e1b2e5f47a8e0e8e8e8e0/fried-rice.jpg',
            ]
        }
        
        return pixabay_images.get(query, [])
    
    def download_batch(self, recipes_config):
        """Download images for multiple recipes"""
        print("\n" + "="*50)
        print("üçΩÔ∏è  Mix & Munch Recipe Image Downloader")
        print("="*50)
        
        results = {}
        
        for dish_name, dish_config in recipes_config.items():
            print(f"\nüì• Downloading images for: {dish_name}")
            
            dish_dir = self.output_dir / dish_name
            dish_dir.mkdir(parents=True, exist_ok=True)
            
            downloaded = []
            
            # Try multiple sources
            sources = [
                ('pexels', self.scrape_pexels(dish_name, 3)),
                ('unsplash', self.scrape_unsplash(dish_name, 3)),
                ('pixabay', self.scrape_pixabay(dish_name, 3)),
            ]
            
            image_count = 0
            for source_name, image_urls in sources:
                for idx, img_url in enumerate(image_urls):
                    if image_count >= 2:  # Limit to 2 per dish
                        break
                    
                    if img_url:
                        filename = dish_dir / f"{dish_name}_{image_count+1}.jpg"
                        result = self.download_image(img_url, filename)
                        if result:
                            downloaded.append(result)
                            image_count += 1
                        time.sleep(0.5)  # Rate limiting
            
            results[dish_name] = {
                'images': downloaded,
                'count': len(downloaded),
                'path': str(dish_dir)
            }
            
            if downloaded:
                print(f"‚úÖ Downloaded {len(downloaded)} images for {dish_name}")
            else:
                print(f"‚ö†Ô∏è  No images downloaded for {dish_name}")
        
        return results
    
    def generate_manifest(self, results):
        """Generate JSON manifest of downloaded images"""
        manifest = {
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),
            'total_recipes': len(results),
            'recipes': results
        }
        
        manifest_path = self.output_dir / 'manifest.json'
        with open(manifest_path, 'w') as f:
            json.dump(manifest, f, indent=2)
        
        print(f"\n‚úÖ Manifest saved to: {manifest_path}")
        return manifest

def main():
    """Main execution"""
    
    # Define recipes to scrape
    recipes_config = {
        'adobo': {
            'name': 'Filipino Adobo',
            'description': 'Chicken/pork stewed in vinegar and soy sauce'
        },
        'fried-rice': {
            'name': 'Garlic Fried Rice',
            'description': 'Rice stir-fried with garlic and egg'
        },
        'sinigang': {
            'name': 'Sinigang',
            'description': 'Pork or shrimp in tamarind broth'
        },
        'kare-kare': {
            'name': 'Kare-Kare',
            'description': 'Stew with peanut sauce'
        },
        'lumpia': {
            'name': 'Lumpia',
            'description': 'Spring rolls with meat filling'
        },
        'pancit': {
            'name': 'Pancit',
            'description': 'Stir-fried noodles'
        },
        'tinola': {
            'name': 'Tinola',
            'description': 'Chicken soup with ginger'
        },
        'bangus': {
            'name': 'Fried Bangus',
            'description': 'Fried milkfish'
        }
    }
    
    # Create scraper
    scraper = FoodImageScraper()
    
    # Download images
    results = scraper.download_batch(recipes_config)
    
    # Generate manifest
    manifest = scraper.generate_manifest(results)
    
    # Print summary
    print("\n" + "="*50)
    print("üìä Download Summary")
    print("="*50)
    total_images = sum(r['count'] for r in results.values())
    print(f"Total images downloaded: {total_images}")
    for dish, data in results.items():
        print(f"  {dish}: {data['count']} images")
    
    print(f"\n‚úÖ Images saved to: {scraper.output_dir}")
    print("‚úÖ Manifest created: public/images/recipes/manifest.json")
    
    return results

if __name__ == '__main__':
    main()
